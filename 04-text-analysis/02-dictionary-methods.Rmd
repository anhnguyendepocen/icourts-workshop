---
title: "Dictionary methods"
author: Pablo Barbera
date: March 30, 2016
output: html_document
---


#### Dictionary methods

A different type of keyword analysis consists on the application of dictionary methods, or lexicon-based approaches to the measurement of tone or the prediction of diferent categories related to the content of the text. 

The most common application is sentiment analysis: using a dictionary of positive and negative words, we compute a sentiment score for each individual document.

Let's apply this technique in the context of our previous example.

```{r}
tweets <- read.csv("../datasets/icourts-tweets.csv", stringsAsFactors=F)
head(tweets)
```

```{r}
# loading lexicon of positive and negative words (from Neal Caren)
lexicon <- read.csv("../datasets/lexicon.csv", stringsAsFactors=F)
pos.words <- lexicon$word[lexicon$polarity=="positive"]
neg.words <- lexicon$word[lexicon$polarity=="negative"]
# a look at a random sample of positive and negative words
sample(pos.words, 10)
sample(neg.words, 10)
```

Before we apply the sentiment analysis, we will need to go through several "preprocessing" steps before it can be passed to a statistical model. We'll use the `quanteda` package  [quanteda](https://github.com/kbenoit/quanteda) here.

The basic unit of work for the `quanteda` package is called a `corpus`, which represents a collection of text documents with some associated metadata. Documents are the subunits of a corpus. We can then convert a corpus into a document-feature matrix using the `dfm` function.

```{r}
#install.packages("quanteda")
library(quanteda)
twcorpus <- corpus(tweets$text)
twdfm <- dfm(twcorpus, ngrams=c(1,3))
```

Note that here we use ngrams -- this will extract all combinations of one, two, and three words (e.g. it will consider both "human", "rights", and "human rights" as tokens in the matrix).

And to take a look at what the data looks like, we can generate a wordcloud

```{r}
plot(twdfm, rot.per=0, scale=c(3.5, .75), max.words=100)
```

We often want to remove words and symbols which are not of interest to our data, such as `http` here. One class of words which is not relevant are called _stopwords_. These are words which are common connectors in a given language (e.g. "a", "the", "is"). Another is punctuation. 


```{r}
twdfm <- dfm(twcorpus, ignoredFeatures=c(
  stopwords("english"), stopwords("spanish"), stopwords("french"), 
  "t.co", "https", "rt", "amp", "http", "t.c", "can"), ngrams=c(1,3))
plot(twdfm, rot.per=0, scale=c(3.5, .75), max.words=100)
```

We can also look at the most common tokens in this dataset with the `topfeatures` function:

```{r}
topfeatures(twdfm, 25)
```

Now we're ready to run the sentiment analysis!

```{r}
# first we construct a dictionary object
mydict <- dictionary(list(negative = neg.words,
                          positive = pos.words))
# apply it to our corpus
sent <- dfm(twcorpus, dictionary = mydict)
# and add it as a new variable
tweets$score <- as.numeric(sent[,2]) - as.numeric(sent[,1])
```

```{r}
# what is the average sentiment score?
mean(tweets$score)
# what is the most positive and most negative tweet?
tweets[which.max(tweets$score),]
tweets[which.min(tweets$score),]
# what is the proportion of positive, neutral, and negative tweets?
tweets$sentiment <- "neutral"
tweets$sentiment[tweets$score<0] <- "negative"
tweets$sentiment[tweets$score>0] <- "positive"
table(tweets$sentiment)
```

We can also disaggregate by groups of tweets, for example according to the hashtag they mention.

```{r}
hts <- c("ICC", "ECHR", "ECJ")

for (h in hts){
  message(h, " -- average sentiment: ",
      round(mean(tweets$score[grep(h, tweets$text, ignore.case=TRUE)]), 4)
    )
}

```

